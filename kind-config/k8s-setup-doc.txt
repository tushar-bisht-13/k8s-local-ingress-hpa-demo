Once kind cluster has been setup follow the below steps:

1) Apply nginx controller from github repo:
	"kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.9.4/deploy/static/provider/kind/deploy.yaml"

- Verify if nginx pods are running:
	"kubectl get pods -n ingress-nginx --watch"
	


docker login -u <username>

docker build -t shadowlord13/page1:latest .
docker run -d -p 8080:80 --name page1-cont shadowlord13/page1:latest

docker build -t shadowlord13/page1:latest .
docker run -d -p 8080:80 --name page2-cont shadowlord13/page2:latest

http://localhost:8080


kubectl apply -f page1-deployment.yaml
kubectl apply -f page1-service.yaml
kubectl apply -f page2-deployment.yaml
kubectl apply -f page2-service.yaml
kubectl apply -f ingress.yaml 
kubectl get all


Errors:

kubectl apply -f ingress.yaml 
Error from server (InternalError): error when creating "ingress.yaml": Internal error occurred: failed calling webhook "validate.nginx.ingress.kubernetes.io": failed to call webhook: Post "https://ingress-nginx-controller-admission.ingress-nginx.svc:443/networking/v1/ingresses?timeout=10s": dial tcp 10.96.149.210:443: connect: connection refused

- This error means the admission webhook for the NGINX Ingress Controller isn't reachable yet. Specifically, this part is failing:
	validate.nginx.ingress.kubernetes.io
The controller's webhook is needed for validating Ingress resources—but it's either not ready or crashed.

Steps to Fix
🔹 1. Check if Ingress Controller Pods Are Ready
	"kubectl get pods -n ingress-nginx"
Make sure all pods are in Running state, especially:
- ingress-nginx-controller
- ingress-nginx-admission-patch (completes once)
If ingress-nginx-controller is CrashLoopBackOff or Pending, the webhook endpoint won’t be reachable.

 2. if ingress-nginx-controller is pending or failed state:
 	describe the pod to get the issue:
 	"kubectl describe pod ingress-nginx-controller-68c9d85487-6tdtg -n ingress-nginx"
 	
 	ingress-nginx-controller pod is in Pending state
This means it can't be scheduled, usually due to insufficient resources or missing node affinity / tolerations in a kind cluster.

Fix: Patch the Deployment to Remove Node Affinity (if needed)
Sometimes the ingress controller expects a specific node label (kubernetes.io/os: linux), which is missing in Kind. You can remove that with:
	"kubectl patch deployment ingress-nginx-controller \
  -n ingress-nginx \
  --type=json \
  -p='[{"op": "remove", "path": "/spec/template/spec/nodeSelector"}]'
"
"kubectl rollout restart deployment ingress-nginx-controller -n ingress-nginx"
"kubectl get pods -n ingress-nginx -w"

Once the pods are in running state apply ingress.yaml:
"kubectl apply -f ingress.yaml"



🔹 2. Wait for Webhook Service to Be Ready
Sometimes this just needs a little time after installing:
	"kubectl get svc -n ingress-nginx"
	output: ingress-nginx-controller-admission   ClusterIP   10.96.149.210   <none>        443/TCP   ...
	
	
	
FOr debugging : issue not able to reach page1 or page 2 through localhost/page1 localhost/page2 after all the setup:
   kubectl get pods
   kubectl get svc\nkubectl describe svc page1-service
   kubectl describe svc page2-service
   kubectl run curlpod --image=radial/busyboxplus:curl -i --tty\n
   kubectl describe ingress html-ingress
   kubectl logs -n ingress-nginx -l app.kubernetes.io/component=controller
   kubectl port-forward svc/page1-service 8080:80
   curl http://localhost:8080
   
   
 Check that the control‑plane container maps port 80
 # find the Docker container ID for the kind control‑plane
docker ps --filter "name=my-kind-cluster-control-plane"

# you should see something like:
# ... 0.0.0.0:80->80/tcp, 0.0.0.0:443->443/tcp ...

   
Quick fix (no rebuild): use port‑forward for now (workaround (test immediately)
kubectl port-forward -n ingress-nginx svc/ingress-nginx-controller 8080:80

# Test in another terminal
curl http://localhost:8080/page1
curl http://localhost:8080/page2


Option A - Permanent fix: recreate the kind cluster with port mappings:
- Delete kind-cluster
	"kind delete cluster --name my-kind-cluster"


 – Create a new cluster with ports mapped and the required node label:
# kind-config.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "ingress-ready=true"
  extraPortMappings:
  - containerPort: 80
    hostPort: 80
    protocol: TCP
  - containerPort: 443
    hostPort: 443
    protocol: TCP
- role: worker

"kind create cluster --name html-demo --config kind-config.yaml"

Option B – Keep the mapping on the worker & run the controller there
If you prefer the mapping on the worker, add the label and force the
Deployment to that node:

# label the worker
kubectl label node my-kind-cluster-worker ingress-ready=true --overwrite

# patch the ingress controller deployment
kubectl patch deploy ingress-nginx-controller -n ingress-nginx \
  --type=json \
  -p='[{"op":"add","path":"/spec/template/spec/nodeSelector","value":{"ingress-ready":"true"}}]'

kubectl rollout restart deploy ingress-nginx-controller -n ingress-nginx

The controller pod will move to the worker (which owns port 80/443) and
curl http://localhost/page1 will start working.

Rule of thumb:
“Map the port on the node that will run the ingress controller, and make that mapping explicit and repeatable in your cluster config.”

For 99 % of local demos, that’s the control‑plane node; for “prod‑like” experiments, move both the mapping and the controller to a worker node.


Install Metric Server in Kind Cluster:

1) # Installs the latest tagged release
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

2 ) Add kind‑friendly flags (skip certs / prefer InternalIP)
By default Metrics Server expects each node’s kubelet to have a public
certificate. In local clusters (kind, Minikube, k3d) that isn’t the case, so we
patch two flags:

 kubectl patch deployment metrics-server -n kube-system \
  --type=json \
  -p='[
    {"op":"add","path":"/spec/template/spec/containers/0/args/-","value":"--kubelet-insecure-tls"},
    {"op":"add","path":"/spec/template/spec/containers/0/args/-","value":"--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname"}
  ]'

3)  Wait until the pod is Ready
kubectl get pods -n kube-system -l "k8s-app=metrics-server"

Expected output: metrics-server-7d8558c6b5-rm62q   1/1   Running   0   20s

4) Check:

kubectl top nodes          # overall stats
kubectl top pod            # per‑pod stats

# Or describe your HPA:
kubectl describe hpa page1-hpa


4 Generate load to prove scaling
Run a quick loop from BusyBox to burn CPU:

"kubectl run loadpoke --image=busybox --restart=Never -i --tty -- sh"
# inside the shell:
while true; do wget -q -O- http://page1-service.default.svc.cluster.local >/dev/null; done

Watch the HPA react:

"watch -n5 kubectl describe hpa page1-hpa"






	

